/*
 *    This file is part of CasADi.
 *
 *    CasADi -- A symbolic framework for dynamic optimization.
 *    Copyright (C) 2010-2014 Joel Andersson, Joris Gillis, Moritz Diehl,
 *                            K.U. Leuven. All rights reserved.
 *    Copyright (C) 2011-2014 Greg Horn
 *
 *    CasADi is free software; you can redistribute it and/or
 *    modify it under the terms of the GNU Lesser General Public
 *    License as published by the Free Software Foundation; either
 *    version 3 of the License, or (at your option) any later version.
 *
 *    CasADi is distributed in the hope that it will be useful,
 *    but WITHOUT ANY WARRANTY; without even the implied warranty of
 *    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
 *    Lesser General Public License for more details.
 *
 *    You should have received a copy of the GNU Lesser General Public
 *    License along with CasADi; if not, write to the Free Software
 *    Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301  USA
 *
 */


#include "nlpsol.hpp"
#include "casadi/core/timing.hpp"

using namespace std;
namespace casadi {

  Nlpsol::Nlpsol(const std::string& name, const XProblem& nlp)
    : FunctionInternal(name), nlp2_(nlp) {

    // Options available in all NLP solvers
    addOption("expand",             OT_BOOLEAN,  false,
              "Expand the NLP function in terms of scalar operations, i.e. MX->SX");
    addOption("hess_lag",            OT_FUNCTION,       GenericType(),
              "Function for calculating the Hessian of the Lagrangian (autogenerated by default)");
    addOption("hess_lag_options",   OT_DICT,           GenericType(),
              "Options for the autogenerated Hessian of the Lagrangian.");
    addOption("grad_lag",           OT_FUNCTION,       GenericType(),
              "Function for calculating the gradient of the Lagrangian (autogenerated by default)");
    addOption("grad_lag_options",   OT_DICT,           GenericType(),
              "Options for the autogenerated gradient of the Lagrangian.");
    addOption("jac_g",              OT_FUNCTION,       GenericType(),
              "Function for calculating the Jacobian of the constraints "
              "(autogenerated by default)");
    addOption("jac_g_options",      OT_DICT,           GenericType(),
              "Options for the autogenerated Jacobian of the constraints.");
    addOption("grad_f",             OT_FUNCTION,       GenericType(),
              "Function for calculating the gradient of the objective "
              "(column, autogenerated by default)");
    addOption("grad_f_options",     OT_DICT,           GenericType(),
              "Options for the autogenerated gradient of the objective.");
    addOption("jac_f",              OT_FUNCTION,       GenericType(),
              "Function for calculating the Jacobian of the objective "
              "(sparse row, autogenerated by default)");
    addOption("jac_f_options",     OT_DICT,           GenericType(),
              "Options for the autogenerated Jacobian of the objective.");
    addOption("iteration_callback", OT_FUNCTION, GenericType(),
              "A function that will be called at each iteration with the solver as input. "
              "Check documentation of Callback.");
    addOption("iteration_callback_step", OT_INTEGER,         1,
              "Only call the callback function every few iterations.");
    addOption("iteration_callback_ignore_errors", OT_BOOLEAN, false,
              "If set to true, errors thrown by iteration_callback will be ignored.");
    addOption("ignore_check_vec",   OT_BOOLEAN,  false,
              "If set to true, the input shape of F will not be checked.");
    addOption("warn_initial_bounds", OT_BOOLEAN,  false,
              "Warn if the initial guess does not satisfy LBX and UBX");
    addOption("eval_errors_fatal", OT_BOOLEAN, false,
              "When errors occur during evaluation of f,g,...,"
              "stop the iterations");
    addOption("verbose_init", OT_BOOLEAN, false,
              "Print out timing information about "
              "the different stages of initialization");

    addOption("defaults_recipes",    OT_STRINGVECTOR, GenericType(), "",
                                                       "qp", true);

    // Enable string notation for IO
    ischeme_ = nlpsol_in();
    oscheme_ = nlpsol_out();

    // Make the ref object a non-refence counted pointer to this (as reference counting would
    // prevent deletion of the object)
    ref_.assignNodeNoCount(this);

    if (nlp.is_sx) {
      nlp_ = Nlpsol::problem2fun<SX>(nlp);
    } else {
      nlp_ = Nlpsol::problem2fun<MX>(nlp);
    }
    alloc(nlp_);
  }

  Nlpsol::~Nlpsol() {
    // Explicitly remove the pointer to this (as the counter would otherwise be decreased)
    ref_.assignNodeNoCount(0);
  }

  Sparsity Nlpsol::get_sparsity_in(int ind) const {
    switch (static_cast<NlpsolInput>(ind)) {
    case NLPSOL_X0:
    case NLPSOL_LBX:
    case NLPSOL_UBX:
    case NLPSOL_LAM_X0:
      return get_sparsity_out(NLPSOL_X);
    case NLPSOL_LBG:
    case NLPSOL_UBG:
    case NLPSOL_LAM_G0:
      return get_sparsity_out(NLPSOL_G);
    case NLPSOL_P:
      return nlp_.sparsity_in(NL_P);
    case NLPSOL_NUM_IN: break;
    }
    return Sparsity();
  }

  Sparsity Nlpsol::get_sparsity_out(int ind) const {
    switch (static_cast<NlpsolOutput>(ind)) {
    case NLPSOL_F:
      return Sparsity::scalar();
    case NLPSOL_X:
    case NLPSOL_LAM_X:
      return nlp_.sparsity_in(NL_X);
    case NLPSOL_LAM_G:
    case NLPSOL_G:
      return nlp_.sparsity_out(NL_G);
    case NLPSOL_LAM_P:
      return get_sparsity_in(NLPSOL_P);
    case NLPSOL_NUM_OUT: break;
    }
    return Sparsity();
  }

  void Nlpsol::init() {
    // Call the initialization method of the base class
    FunctionInternal::init();

    // Get dimensions
    nx_ = nnz_out(NLPSOL_X);
    np_ = nnz_in(NLPSOL_P);
    ng_ = nnz_out(NLPSOL_G);

    // Find out if we are to expand the NLP in terms of scalar operations
    bool expand = option("expand");
    if (expand) {
      log("Expanding NLP in scalar operations");
      Function f = nlp_.expand(nlp_.name());
      f.copyOptions(nlp_, true);
      f.init();
      nlp_ = f;
    }

    if (hasSetOption("iteration_callback")) {
      fcallback_ = option("iteration_callback");

      // Consistency checks
      casadi_assert(!fcallback_.isNull());
      casadi_assert_message(fcallback_.n_out()==1 && fcallback_.numel_out()==1,
                            "Callback function must return a scalar");
      casadi_assert_message(fcallback_.n_in()==n_out(),
                            "Callback input signature must match the NLP solver output signature");
      for (int i=0; i<n_out(); ++i) {
        casadi_assert_message(fcallback_.size_in(i)==size_out(i),
                              "Callback function input size mismatch");
        // TODO(@jaeandersson): Wrap fcallback_ in a function with correct sparsity
        casadi_assert_message(fcallback_.sparsity_in(i)==sparsity_out(i),
                              "Not implemented");
      }

      // Allocate temporary memory
      alloc(fcallback_);
    }

    callback_step_ = option("iteration_callback_step");
    eval_errors_fatal_ = option("eval_errors_fatal");

  }

  void Nlpsol::checkInputs(void* mem) const {
    // Skip check?
    if (!inputs_check_) return;

    const double inf = std::numeric_limits<double>::infinity();
    bool warn_initial_bounds = option("warn_initial_bounds");

    // Detect ill-posed problems (simple bounds)
    for (int i=0; i<nx_; ++i) {
      casadi_assert_message(!(lbx(i)==inf || lbx(i)>ubx(i) || ubx(i)==-inf),
                            "Ill-posed problem detected (x bounds)");
      if (warn_initial_bounds && (x0(i)>ubx(i) || x0(i)<lbx(i))) {
        casadi_warning("Nlpsol: The initial guess does not satisfy LBX and UBX. "
                       "Option 'warn_initial_bounds' controls this warning.");
        break;
      }
    }

    // Detect ill-posed problems (nonlinear bounds)
    for (int i=0; i<ng_; ++i) {
      casadi_assert_message(!(lbg(i)==inf || lbg(i)>ubg(i) || ubg(i)==-inf),
                            "Ill-posed problem detected (g bounds)");
    }
  }

  Function& Nlpsol::gradF() {
    if (gradF_.isNull()) {
      gradF_ = getGradF();
      alloc(gradF_);
    }
    return gradF_;
  }

  Function& Nlpsol::jacF() {
    if (jacF_.isNull()) {
      jacF_ = getJacF();
      alloc(jacF_);
    }
    return jacF_;
  }

  Function Nlpsol::getJacF() {
    Function jacF;
    if (hasSetOption("jac_f")) {
      jacF = option("jac_f");
    } else {
      log("Generating objective jacobian");
      const bool verbose_init = option("verbose_init");
      if (verbose_init)
        userOut() << "Generating objective Jacobian...";
      Timer time0 = getTimerTime();
      jacF = nlp_.jacobian(NL_X, NL_F);
      DiffTime diff = diffTimers(getTimerTime(), time0);
      stats_["objective jacobian gen time"] = diffToDict(diff);
      if (verbose_init)
        userOut() << "Generated objective Jacobian in " << diff.user << " seconds.";
      log("Jacobian function generated");
    }
    if (hasSetOption("jac_f_options")) {
      jacF.setOption(option("jac_f_options"));
    }
    jacF.init();
    casadi_assert_message(jacF.n_in()==GRADF_NUM_IN,
                          "Wrong number of inputs to the gradient function. "
                          "Note: The gradient signature was changed in #544");
    casadi_assert_message(jacF.n_out()==GRADF_NUM_OUT,
                          "Wrong number of outputs to the gradient function. "
                          "Note: The gradient signature was changed in #544");
    log("Objective gradient function initialized");
    return jacF;
  }

  Function Nlpsol::getGradF() {
    Function gradF;
    if (hasSetOption("grad_f")) {
      gradF = option("grad_f");
    } else {
      log("Generating objective gradient");
      const bool verbose_init = option("verbose_init");
      if (verbose_init)
        userOut() << "Generating objective gradient...";
      Timer time0 = getTimerTime();
      gradF = nlp_.gradient(NL_X, NL_F);
      DiffTime diff = diffTimers(getTimerTime(), time0);
      stats_["objective gradient gen time"] = diffToDict(diff);
      if (verbose_init)
        userOut() << "Generated objective gradient in " << diff.user << " seconds.";
      log("Gradient function generated");
    }
    if (hasSetOption("grad_f_options")) {
      gradF.setOption(option("grad_f_options"));
    }
    gradF.init();
    casadi_assert_message(gradF.n_in()==GRADF_NUM_IN,
                          "Wrong number of inputs to the gradient function. "
                          "Note: The gradient signature was changed in #544");
    casadi_assert_message(gradF.n_out()==GRADF_NUM_OUT,
                          "Wrong number of outputs to the gradient function. "
                          "Note: The gradient signature was changed in #544");
    log("Objective gradient function initialized");
    return gradF;
  }

  Function& Nlpsol::jacG() {
    if (jacG_.isNull()) {
      jacG_ = getJacG();
      alloc(jacG_);
    }
    return jacG_;
  }

  Function Nlpsol::getJacG() {
    Function jacG;

    // Return null if no constraints
    if (ng_==0) return jacG;

    if (hasSetOption("jac_g")) {
      jacG = option("jac_g");
    } else {
      log("Generating constraint Jacobian");
      const bool verbose_init = option("verbose_init");
      if (verbose_init)
        userOut() << "Generating constraint Jacobian...";
      Timer time0 = getTimerTime();
      jacG = nlp_.jacobian(NL_X, NL_G);
      DiffTime diff = diffTimers(getTimerTime(), time0);
      stats_["constraint jacobian gen time"] = diffToDict(diff);
      if (verbose_init)
        userOut() << "Generated constraint Jacobian in " << diff.user << " seconds.";
      log("Jacobian function generated");
    }
    if (hasSetOption("jac_g_options")) {
      jacG.setOption(option("jac_g_options"));
    }
    jacG.init();
    casadi_assert_message(jacG.n_in()==JACG_NUM_IN,
                          "Wrong number of inputs to the Jacobian function. "
                          "Note: The Jacobian signature was changed in #544");
    casadi_assert_message(jacG.n_out()==JACG_NUM_OUT,
                          "Wrong number of outputs to the Jacobian function. "
                          "Note: The Jacobian signature was changed in #544");
    log("Jacobian function initialized");
    return jacG;
  }

  Function& Nlpsol::gradLag() {
    if (gradLag_.isNull()) {
      gradLag_ = getGradLag();
      alloc(gradLag_);
    }
    return gradLag_;
  }

  Function Nlpsol::getGradLag() {
    Function gradLag;
    if (hasSetOption("grad_lag")) {
      gradLag = option("grad_lag");
    } else {
      log("Generating/retrieving Lagrangian gradient function");
      const bool verbose_init = option("verbose_init");
      if (verbose_init)
        userOut() << "Generating/retrieving Lagrangian gradient function...";
      Timer time0 = getTimerTime();
      gradLag = nlp_.derivative(0, 1);
      DiffTime diff = diffTimers(getTimerTime(), time0);
      stats_["grad lag gen time"] = diffToDict(diff);
      if (verbose_init)
        userOut() << "Generated/retrieved Lagrangien gradient in "
                  << diff.user << " seconds.";
      log("Gradient function generated");
    }
    if (hasSetOption("grad_lag_options")) {
      gradLag.setOption(option("grad_lag_options"));
    }
    gradLag.init();
    log("Gradient function initialized");
    return gradLag;
  }

  Function& Nlpsol::hessLag() {
    if (hessLag_.isNull()) {
      hessLag_ = getHessLag();
      alloc(hessLag_);
    }
    return hessLag_;
  }

  Function Nlpsol::getHessLag() {
    Function hessLag;
    if (hasSetOption("hess_lag")) {
      hessLag = option("hess_lag");
    } else {
      Function& gradLag = this->gradLag();
      log("Generating Hessian of the Lagrangian");
      const bool verbose_init = option("verbose_init");
      if (verbose_init)
        userOut() << "Generating Hessian of the Lagrangian...";
      Timer time0 = getTimerTime();
      hessLag = gradLag.jacobian(NL_X, NL_NUM_OUT+NL_X, false, true);
      DiffTime diff = diffTimers(getTimerTime(), time0);
      stats_["hess lag gen time"] = diffToDict(diff);
      if (verbose_init)
        userOut() << "Generated Hessian of the Lagrangian in "
                  << diff.user << " seconds.";
      log("Hessian function generated");
    }
    if (hasSetOption("hess_lag_options")) {
      hessLag.setOption(option("hess_lag_options"));
    }
    hessLag.init();
    casadi_assert_message(hessLag.n_in()==HESSLAG_NUM_IN,
                          "Wrong number of inputs to the Hessian function. ");
    casadi_assert_message(hessLag.n_out()==HESSLAG_NUM_OUT,
                          "Wrong number of outputs to the Hessian function. ");
    log("Hessian function initialized");
    return hessLag;
  }

  Sparsity& Nlpsol::spHessLag() {
    if (spHessLag_.isNull()) {
      spHessLag_ = getSpHessLag();
    }
    return spHessLag_;
  }

  Sparsity Nlpsol::getSpHessLag() {
    Sparsity spHessLag;
    if (false /*hasSetOption("hess_lag_sparsity")*/) {
      // NOTE: No such option yet, need support for GenericType(Sparsity)
      //spHessLag = option("hess_lag_sparsity");
    } else {
      Function& gradLag = this->gradLag();
      log("Generating Hessian of the Lagrangian sparsity pattern");
      const bool verbose_init = option("verbose_init");
      if (verbose_init)
        userOut() << "Generating Hessian of the Lagrangian sparsity pattern...";
      Timer time0 = getTimerTime();
      spHessLag = gradLag.sparsity_jac(NL_X, NL_NUM_OUT+NL_X, false, true);
      DiffTime diff = diffTimers(getTimerTime(), time0);
      stats_["hess lag sparsity time"] = diffToDict(diff);
      if (verbose_init)
        userOut() << "Generated Hessian of the Lagrangian sparsity pattern in "
                  << diff.user << " seconds.";
      log("Hessian sparsity pattern generated");
    }
    return spHessLag;
  }

  std::map<std::string, Nlpsol::Plugin> Nlpsol::solvers_;

  const std::string Nlpsol::infix_ = "nlpsol";

  DM Nlpsol::getReducedHessian() {
    casadi_error("Nlpsol::getReducedHessian not defined for class "
                 << typeid(*this).name());
    return DM();
  }

  void Nlpsol::setOptionsFromFile(const std::string & file) {
    casadi_error("Nlpsol::setOptionsFromFile not defined for class "
                 << typeid(*this).name());
  }

  double Nlpsol::default_in(int ind) const {
    switch (ind) {
    case NLPSOL_LBX:
    case NLPSOL_LBG:
      return -std::numeric_limits<double>::infinity();
    case NLPSOL_UBX:
    case NLPSOL_UBG:
      return std::numeric_limits<double>::infinity();
    default:
      return 0;
    }
  }

  void Nlpsol::eval(const double** arg, double** res, int* iw, double* w, void* mem) {
    // Reset the solver, prepare for solution
    reset(mem, arg, res, iw, w);

    // Work vectors for evaluation
    arg_ = arg;
    res_ = res;
    iw_ = iw;
    w_ = w;

    // Solve the NLP
    solve(mem);
  }

  void Nlpsol::reset(void* mem, const double**& arg, double**& res, int*& iw, double*& w) {
    // Get input pointers
    x0_ = arg[NLPSOL_X0];
    p_ = arg[NLPSOL_P];
    lbx_ = arg[NLPSOL_LBX];
    ubx_ = arg[NLPSOL_UBX];
    lbg_ = arg[NLPSOL_LBG];
    ubg_ = arg[NLPSOL_UBG];
    lam_x0_ = arg[NLPSOL_LAM_X0];
    lam_g0_ = arg[NLPSOL_LAM_G0];
    arg += NLPSOL_NUM_IN;

    // Get output pointers
    x_ = res[NLPSOL_X];
    f_ = res[NLPSOL_F];
    g_ = res[NLPSOL_G];
    lam_x_ = res[NLPSOL_LAM_X];
    lam_g_ = res[NLPSOL_LAM_G];
    lam_p_ = res[NLPSOL_LAM_P];
    res += NLPSOL_NUM_OUT;
  }

} // namespace casadi
