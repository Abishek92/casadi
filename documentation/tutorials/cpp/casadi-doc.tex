\documentclass[a4paper,12pt]{book}
\usepackage{graphicx}
\usepackage{tabularx}
\usepackage{a4wide}
%\usepackage[bookmarks=false,breaklinks=true,pdfstartview=Fit]{hyperref}
\usepackage{amsfonts}
\usepackage{amsmath}
%\usepackage{listings}
%\usepackage{color}
\usepackage[usenames,dvipsnames]{color}
\usepackage{html}
%\usepackage{hyperref}
\usepackage{verbatim}

% \#c4ebff;
\newcommand{\codebegin}{
\begin{rawhtml}
<div style="color: black; background-color: \#a9b8cb;  border-style: dotted; border-width: 1px;" >
\end{rawhtml}
}
\newcommand{\codeend}{
\begin{rawhtml}
</div>
\end{rawhtml}
}


\author{Joel Andersson, Attila Kozma}
\title{CasADi C++ Tutorial }
\begin{document}
%\htmlinfo*
%\sffamily
%\titlepage
\maketitle
%\clearpage
\begin{latexonly}
\tableofcontents
\end{latexonly}
\clearpage

\chapter{Introduction}
This document gives a brief tutorial on the CasADi software package and on the interfaces that comes with it. We aim at
introducing the most important capabilities of what CasADi can do. Our fundemental motivation is to provide open-source
software for people working with symbolic systems, differential equations and dynamic optimization.
\par Our discussion is mostly program code driven, meaning that
we do not even try to provide mathematical background knowledge, the writers assume that the reader has already a fair knowledge
of C++ programming, theory of differential equations and optimization theory. The goal of this document is to make the reader
 familiar with
the syntax of CasADi and provide easily available building blocks to build dynamic optimization sofware.
\par
After reading this tutorial the reader will be able to represent and derivate mathematical functions in a symbolic system with
code generation features, to
set up and solve ordinary differential equations with sensitivities.
\chapter{Installation}
In this section we introduce step by step the installation procedure of CasADi. First we detail what software packages need to
be installed in order to use CasADi.
\section{Software requirements}
CasADi consists of a symbolic core, which is strongly used by the interfaces that are shipped with it. Now we summarize
what the dependencies of each are. As a general rule you will always need a C++ compiler (tested with gcc 4.4.3) and
CMake (tested with version 2.8.2).
\begin{center}
\small
\begin{tabular}{|l|l|}
\hline
\textbf{Package/interface} & \textbf{Dependency}\\
\hline
CasADi core & None \\
\hline
CSparse interface & CSparse (comes with CasADi)\\
\hline
Ipopt interface & Ipopt\\
\hline
KNITRO interface & KNITRO\\
\hline
LAPACK interface & library with LAPACK api\\
\hline
LiftOpt interface & LiftOpt\\
\hline
Sundials interface & SUNDIALS \\
\hline
SuperLU interface & SuperLU, library with BLAS api\\
\hline
CasADi for python & SWIG \\
\hline
\end{tabular}
\end{center}
\section{Compilation}
First, you have to get CasADi from \htmladdnormallink{here}{http://www.casadi.org/} or check out the svn repository from SourceForge.
\par
\codebegin
\begin{verbatim}
svn co https://casadi.svn.sourceforge.net/svnroot/casadi/trunk casadi
\end{verbatim}
\codeend\\
Set the \texttt{CMAKE_PREFIX_PATH} to inform CMake where the dependecies are. 
For example if Sundials headers and libraries are installed under \texttt{\$HOME/local/}, then type
\par
\codebegin{
\begin{verbatim}
export CMAKE_PREFIX_PATH=$HOME/local/
\end{verbatim}
\codeend\\
\par
Go to directory of the source tree, create a directory called \texttt{build} where all compilation-related files will be created.
\par
\codebegin
\begin{verbatim}
cd casadi; mkdir build; cd build
\end{verbatim}
\codeend\\
Generate the \texttt{Makefile} by typing
\par
\codebegin
\begin{verbatim}
cmake ..
\end{verbatim}
\codeend\\
Check the output of CMake if the dependencies are correctly located. The interfaces without underlying libraries and headers won't be compiled.
Now compile the C++ libraries.
\par
\codebegin
\begin{verbatim}
make
\end{verbatim}
\codeend\\
If you wish to use the python interface you can compile it with the \texttt{python} target.
\par
\codebegin
\begin{verbatim}
make python
\end{verbatim}
\codeend\\

\chapter{Symbolic Core}
In CasADi a very powerful symbolic abstraction of mathematical functions is implemented.
In this chapter we will learn how to create general functions out of our model equations, 
how to evaluate them and optionally their derivatives. We also give some instructional
examples to make it more easily understandable.
\section{Symbolic expressions}
Normally, mathematical expressions represent operations between variables and parameters.
In order to build up our models, we need to have a computer-based abstraction of these. In CasADi
the \texttt{\htmladdnormallink{SX}{http://casadi.sourceforge.net/api/html/d2/db3/classCasADi_1_1SX.html}} class provides us exactly these building blocks. We create a simple expression:
\par
\codebegin
\begin{verbatim}
     1  #include "casadi/sx/sx.hpp"
     2  using namespace CasADi;
     3  int main(int argc, char* argv[]){
     4     SX x("x");
     5     SX expr;
     6     cout << "x: " << x << endl;
     7     cout << "expr: " << expr << endl;
     8     expr = 3.0 * x + pow(x, 2) - sin(x);
     9     cout << "expr: " << expr << endl;
    10     SX y("y");
    11     expr += y / 2 / x;
    12     cout << "expr: " << expr << endl;
    13  }
\end{verbatim}
\codeend\\
And the output is
\par
\codebegin
\begin{verbatim}
   x: x
   expr: nan
   expr: (((3*x)+(x*x))-sin(x))
   expr: ((((3*x)+(x*x))-sin(x))+((y/2)/x))
\end{verbatim}
\codeend\\
On the 1st line we include the header file containing the \texttt{SX} class, on the 4th the \texttt{x} object
is created and from here on the string \texttt{"x"} is assigned to this variable. We can also create 
\texttt{SX} objects by invoking the default constructor (as we do on line 5), although this variable is not initialized to any symbolic
expression at this time. The \texttt{expr} object is initialized on the 8th line, while from the 10th line on we extend our expression
with a \texttt{y} variable. One can always check whether an expression is correct by simply printing it to the standard output.\\
With the use of \texttt{SX} objects and the \texttt{+ - * / += -= *= /= exp log sqrt sin pow} etc. operations we can build up a
wide range of mathemetical expressions.
\par
We can also create a vector of expressions if needed, although each element must be initialized that may be
carried out by the \texttt{make\_symbolic} function. Now we demonstrate how this works.
\par
\codebegin
\begin{verbatim}
     1  #include "casadi/sx/sx.hpp"
     2  #include "casadi/sx/sx_tools.hpp"
     3  
     4  ...
     5  vector<SX> z(10);
     6  make_symbolic(z.begin(), z.end(), "z");
     7  cout << "z[4]: " << z[4] << endl;
     8  ...
\end{verbatim}
\codeend\\
After invoking \texttt{make\_symbolic} on the 6th line the elements of \texttt{z} are assigned to \texttt{z\_0, z\_1, $\dots$} and can be used later on in modelling.
\section{Functions}
Now that we can build up mathematical expressions we would like to evaluate them symbolically or numerically. We will see how this can be
done with the \texttt{SXFunction} object, which can represent mathematical functions that has prototype
 $\mathbb{R} \rightarrow \mathbb{R}$, $\mathbb{R}^n \rightarrow \mathbb{R}$,
$\mathbb{R}^n \rightarrow \mathbb{R}^m $ or $\mathbb{R}^{n_1 \times n_2} \rightarrow \mathbb{R}^{m_1 \times m_2}$.
In the constructor of \texttt{\htmladdnormallink{SXFunction}{http://casadi.sourceforge.net/api/html/d2/d58/classCasADi_1_1SXFunction.html}} one has to give two arguments. The first argument should contain all the variables
that the function will depend on, and the second argument stores the function definition itself. The data type of the arguments may be various
depending on the prototype of the function that is to be represented. Let's see some examples.
\subsection*{Numerical evaluation}
Most often we would like to compute the value of a function in a certain point. In this subsection we will learn how to do that
 in case of different function prototypes.
First we create and evaluate a simple scalar-scalar function numerically.
\par
\codebegin
\begin{verbatim}
     1  #include "casadi/sx/sx.hpp"
     2  ...
     3  SX x("x");
     4  SX f_expr = x * x - 3 * x + 2;
     5  SXFunction f(x, f_expr);
     6  f.init();
     7  double d = 10.0;
     8  f.setInput(d);
     9  f.evaluate();
    10  cout << "Function value in x = " << d << " : " << f.output() << "\n";
\end{verbatim}
\codeend\\
On the 4th line we create the expression that will represent our function. The \texttt{SXFunction}
object is created on the 5th line with the variable \texttt{x} and the \texttt{f\_expr} object.
Before passing any input to the function, we have to initialize it on line 6. Then we set the input
of the function, evaluate it and finally print the result.
\par\noindent
Now we create a vector-scalar function that calculates the 2-norm of its argument.
\par
\codebegin
\begin{verbatim}
     1  vector<SX> x(4);
     2  make_symbolic(x.begin(), x.end(), "x");
     3  SX f_expr = 0;
     4  for(int i = 0; i < x.size(); ++i){
     5     f_expr += x[i] * x[i];
     6  }
     7  f_expr = sqrt(f_expr);
     8  SXFunction f(x, f_expr);
     9  f.init();
    10  vector<double> d(x.size());
    11  fill(d.begin(), d.end(), 3);
    12  f.setInput(d);
    13  f.evaluate();
    14  cout << "Function value: " << f.output() << "\n";
\end{verbatim}
\codeend
The principle is exactly the same, the only difference is that now we pass a \texttt{std::vector<SX>} in the first argument
on line 8 and we provide a \texttt{std::vector<double>} as an input on line 12.
\par\noindent
Now we define and evaluate a vector-vector function.
\par 
\codebegin
\begin{verbatim}
     1  vector<SX> x(3);
     2  make_symbolic(x.begin(), x.end(), "x");
     3  vector<SX> f_expr(2);
     4  fill(f_expr.begin(), f_expr.end(), 0);
     5  f_expr[0] = x[0] + 2 * x[1];
     6  f_expr[1] = x[1] + 4 * exp(x[2]);
     7  SXFunction f(x, f_expr);
     8  f.init();
     9  vector<double> d(3);
    10  fill(d.begin(), d.end(), 2);
    11  f.setInput(d);
    12  f.evaluate();
    13  vector<double> value = f.output();
    14  cout << "[" << value.size() << "](";
    15  for(int i = 0; i < value.size(); ++i){
    16     cout << value[i] << ",";
    17  }
    18  cout << "\b)\n";
\end{verbatim}
\codeend\\
This time both arguments of the constructor on line 7 are \texttt{std::vector<SX>}
 and the return value of the
 \texttt{output()} member function is a \texttt{std::vector<double>} (line 13).
\par\noindent
As the last example let's see a piece of code, where the function depends on a  \texttt{std::vector<std::vector<SX> >}. 
Note that this covers the case when our function depends on a matrix.
\par
\codebegin
\begin{verbatim}
     1  vector<vector<SX> > variables;
     2  vector<SX> x(3);
     3  make_symbolic(x.begin(), x.end(), "x");
     4  variables.push_back(x);
     5  vector<SX> y(2);
     6  make_symbolic(y.begin(), y.end(), "y");
     7  variables.push_back(y);
     8  vector<SX> f_expr(2);
     9  f_expr[0] = x[0] + x[2] - sin(y[0]);
    10  f_expr[1] = cos(x[1] - y[1]);
    11  SXFunction f(variables, f_expr);
    12  f.init();
    13  vector<double> d0(3);
    14  fill(d0.begin(), d0.end(), 6.0);
    15  vector<double> d1(2);
    16  fill(d1.begin(), d1.end(), 3.0);
    17  f.setInput(d0, 0);
    18  f.setInput(d1, 1);
    19  f.evaluate();
    20  vector<double> value = f.output();
    21  cout << "[" << value.size() << "](";
    22  for(int i = 0; i < value.size(); ++i){
    23     cout << value[i] << ",";
    24  }
    25  cout << "\b)\n";
\end{verbatim}
\codeend\\
In variable \texttt{variables} we collect vectors of \texttt{SX} objects. On line 17 and 18 you can notice
a new argument (which was always 0 until now) that addresses an input vector. In this example on the first coordinate
we have a 3 long vector, on the second we have a 2 long vector, which correspond to the function definition (line 11).
It is important to understand the concept of functions depending on matrices, because, as it will be shown later on (see
Chapter \ref{chapter:integrators} about integrators),
integrators are exactly of this type.
 \subsection*{Symbolic evaluation}
 \par\noindent
 One might also wish to replace certain variables in a symbolic expression with some other variables
  or with more complex expressions. This may be carried out easily by the use of symbolic evaluation, we use
  the \texttt{eval(...)} member function of the \texttt{SXFunction} class.
\par
\codebegin
\begin{verbatim}
     1  vector<SX> x(3);
     2  make_symbolic(x.begin(), x.end(), "x");
     3  vector<SX> f_expr(2);
     4  f_expr[0] = x[0] + 5 * x[1];
     5  f_expr[1] = x[0] + cos(x[2]);
     6  SXFunction f(x, f_expr);
     7  f.init();
     8  vector<SX> y = x;
     9  y[0] = x[1] + pow(x[2], 3);
    10  vector<SX> f_expr_subs = f.eval(y);
    11  cout << f_expr[0] << " --> " << f_expr_subs[0] << endl;
    12  cout << f_expr[1] << " --> " << f_expr_subs[1] << endl;
\end{verbatim}
\codeend\\
First we create a function depending on the vector \texttt{x} (line 1 \-- 7), then we create a vector of \texttt{SX} objects, whose
size correspond to the function definition and we modify the first element by an arbitrary expression (line 9). As a result we get
back the function expression in which the substitution has already been committed.
\section{Calculating derivatives}
In this section we will learn how to calculate first-order derivatives of general functions. Within CasADi derivatives of general
functions may be calculated in two ways. First, the derivatives are calculated numerically based on automatic differentiation,
 which has two modes forward and backward. Second, knowing the symbolic representation of the functions one may differentiate
 symbolically.

\subsection*{Forward differentiation}
 In the forward mode one can calculate the directional derivative
$J p$, where $J$ is the Jacobian of a $f: \mathbb{R}^n \rightarrow \mathbb{R}^m$ and $p \in \mathbb{R}^m$ is the direction.
In the following code we calculate the full Jacobian providing multiple directions.
\par
\codebegin
\begin{verbatim}
     1  vector<SX> x(3);
     2  make_symbolic(x.begin(), x.end(), "x");
     3  vector<SX> f_expr(2);
     4  f_expr[0] = x[0] + 5 * x[1];
     5  f_expr[1] = x[0] + cos(x[2]);
     6  SXFunction f(x, f_expr);
     7  f.setOption("number_of_fwd_dir", (int)x.size());
     8  f.init();
     9  vector<double> d(x.size());
    10  fill(d.begin(), d.end(), 4.0);
    11  f.setInput(d);
    12  vector<double> fwd_seed(x.size());
    13  fill(fwd_seed.begin(), fwd_seed.end(), 0);
    14  for(int i = 0; i < x.size(); ++i){
    15     fwd_seed[i] = 1;
    16     f.setFwdSeed(fwd_seed, 0, i);
    17     fwd_seed[i] = 0;
    18  }
    19  f.evaluate(1, 0);
    20  cout << "Function value: " << f.output() << endl;
    21  vector<double> jacobian_column;
    22  for(int i = 0; i < x.size(); ++i){
    23     jacobian_column = f.fwdSens(0, i);
    24     cout << "[" << jacobian_column.size() << "](";
    25     for(int j = 0; j < jacobian_column.size(); ++j){
    26        cout << jacobian_column[j] << ",";
    27     }
    28     cout << "\b)\n";
    29  }
\end{verbatim}
\codeend\\
The first new thing is on line 7, we set an option called \texttt{number\_of\_fwd\_dir} to the number
of the forward directions in which we would like to differentiate. On line 16 we are inside a for loop
and we provide the directions; now only columns of the identity matrix. The way of evaluation is also
slightly different (linr 19), so far we haven't given any arguments. The first argument indicates that we would like
forward derivatives to be evaluated as well as function evaluation, the second is just the same,
 but with adjoint derivatives. We can collect our
directional derivatives by the \texttt{fwdSens(...)} member function (line 23). Note that the first argument addresses
the the matrix input. Since we have only vector input now, this is always zero.
\subsection*{Backward (adjoint) differentiation}
In this mode of derivative calculation one can calculate $J^T p$. Mutiple directions are not yet supported by CasADi.
The code is as follows:
\par
\codebegin
\begin{verbatim}
     1  vector<SX> x(3);
     2  make_symbolic(x.begin(), x.end(), "x");
     3  vector<SX> f_expr(2);
     4  f_expr[0] = x[0] + 5 * x[1];
     5  f_expr[1] = x[0] + cos(x[2]);
     6  SXFunction f(x, f_expr);
     7  f.setOption("number_of_adj_dir", 1);
     8  f.init();
     9  vector<double> d(x.size());
    10  fill(d.begin(), d.end(), 4.0);
    11  f.setInput(d);
    12  vector<double> adj_seed(f_expr.size());
    13  fill(adj_seed.begin(), adj_seed.end(), 1);
    14  f.setAdjSeed(adj_seed, 0, 0);
    15  f.evaluate(0, 1);
    16  cout << "Function value: " << f.output() << endl;
    17  cout << "Adjoint derivative: " << f.adjSens(0, 0) << endl;
\end{verbatim}
\codeend\\
This time we set the option \texttt{number\_of\_adj\_dir} (greater than 1 throws an error) and we invoke \texttt{evaluate(0, 1)}.
The adjoint derivative we can access by the \texttt{adjSens(...)} method.
\par
\subsection*{Symbolic differentiation}
The second way one can differentiate is by symbolic manipulation. The result of this approach is nothing else than a
function symbolically representing the first derivative of the original function. The following example demonstrates this.
\par
\codebegin
\begin{verbatim}
     1  vector<SX> variables;
     2  vector<SX> x(3);
     3  make_symbolic(x, "x");
     4  variables = x;
     5  vector<SX> f_expr(2);
     6  f_expr[0] = x[0] + x[2] - sin(x[0]);
     7  f_expr[1] = cos(x[1] - x[2] * x[0]);
     8  SXFunction f(variables, f_expr);
     9  f.init();
    10  SXFunction f_der = f.jacobian();
    11  cout << "f: " << f.outputSX() << "\n"; 
    12  cout << "f_der: " << f_der.outputSX() << "\n";
    13  vector<double> d0(3);
    14  fill(d0.begin(), d0.end(), 6.0);
    15  f_der.init();
    16  f_der.setInput(d0);
    17  f_der.evaluate();
    18  cout << "Jacobian: " << f_der.output() << "\n"; 
\end{verbatim}
\codeend\\
The code is very self-explanatory, we create a vector-vector function (line 1-9) and invoke the \texttt{jacobian()} method, which
returns an \texttt{SXFunction} object. This object, since it's a function itself, can be evaluated and differentiated again.
This way we may calculate higher-order derivatives.
\chapter{Integrators \label{chapter:integrators}}
In this chapter we will discuss a special family of functions: integrators. We would like to solve explicit and implicit 
Ordinary Differential Equations (ODE), quadrature formulas together with sensitivities. CasADi comes with well established
interface to state-of-the-art ODE solvers like CVODES and IDAS from the Sundials suite.
\section{Integration of explicit ODEs}
In this section we will study how to use integrators and calculate sensitivites. Most of the things are already known from previous sections
since integrators transparently behave like functions. We are concerned with the following explicit ODE.
\begin{align}
\dot{x} &= f(x(t), p)\\
\dot{y} &= g(x(t), p)\\
x(0) &= x_0
\end{align}
In the following piece of code we show how easy it is to solve ODEs with quadratures.
\par
\codebegin
\begin{verbatim}
     1  SX t; SX u; SX s; SX v; SX m;
     2  vector<SX> ode(3);
     3  ode[0] = v;
     4  ode[1] = (u - 0.02 * v * v) / m;
     5  ode[2] = -0.01 * u * u;
     6  vector<SX> quad(2);
     7  quad[0] = s;
     8  quad[1] = 0.45 * m * v * v;
     9  vector<vector<SX> > ode_vars(3);
    10  ode_vars[0].push_back(t);
    11  ode_vars[1].push_back(s); ode_vars[1].push_back(v); ode_vars[1].push_back(m);
    12  ode_vars[2].push_back(u);
    13  SXFunction ode_func(ode_vars, ode);
    14  SXFunction quad_func(ode_vars, quad);
    15  CVodesIntegrator integrator(ode_func, quad_func);
    16  integrator.init();
    17  integrator.setInput(0.0, INTEGRATOR_T0);
    18  integrator.setInput(1.0, INTEGRATOR_TF);
    19  vector<double> x0(3 + 2);
    20  fill(x0.begin(), x0.end(), 0.0);
    21  x0[0] = 0; x0[1] = 1; x0[2] = 10;
    22  integrator.setInput(x0, INTEGRATOR_X0);
    23  vector<double> p(1);
    24  p[0] = 0.15;
    25  integrator.setInput(p, INTEGRATOR_P);
    26  integrator.evaluate();
    27  vector<double> xf = integrator.output(INTEGRATOR_XF);
    28  cout << "Solution of ODE: (";
    29     for(int j = 0; j < 3; ++j){
    30        cout << xf[j] << ",";
    31     }
    32     cout << "\b)\n";
    33  cout << "Solution of quadrature: (";
    34     for(int j = 3; j < 5; ++j){
    35        cout << xf[j] << ",";
    36     }
    37     cout << "\b)\n";
\end{verbatim}
\codeend\\
First of all we introduce a time variable (\texttt{t}), a control parameter (\texttt{u}) and three dynamic variables(\texttt{s, v, m}).
Their data type do not differ, but their use will give them different meaning. We build up our ODE equations (line 2-5) and quadrature formula
(line 6-8). Then we collect all our variables in a vector of vectors. In case of integrators one must respect the following rules regarding the
function variables. The first element
is a vector containing only the time variable (line 10). The second element is always the vector of all dynamic variables (line 11).
The third element contains the parameters (now we have only one control variable, line 12). Afterwards, we have to define two functions corresponding
to our ODE (line 13) and quadrature equations (line 14). Now everything is ready to instantiate the \texttt{CVodesIntegrator class}, which is an interface
to the CVODES algorithm. If we have no quadratures then we simply pass only the ODE equations (line 15). If we don't set any options a BDF method is used
to carry out integration. Later on we detail what options integrators and especially the \texttt{CVodesIntegrator} class has.
\par After initialization we set the time interval and initial value of the problem. We use the already known \texttt{setInput} method and the
\texttt{INTEGRATOR\_T0} and \texttt{INTEGRATOR\_TF} enum types to address the initial and final time (line 17-18). The initial value of the dynamic
variables is set by the use of \texttt{INTERGRATOR\_X0}. Note that CVODES handles quadratures by introducing dummy dynamic variables and thus the initial
value must have length of 3 + 2 = 5 in our example (line 19-22). The parameters (in our case \texttt{u}) should also be assigned a value, this may be done
using the \texttt{INTEGRATOR\_P} enum type (line 25).After evaluation we can access the solution of the ODE and quadratures by the \texttt{output} method
saying that we would like the value of the dynamic variables at the final time \texttt{INTEGRATOR\_XF} (see line 27).
%\subsection*{Evaluation of integrators}
\subsection*{Evaluation of forward and backward sensitivities}
As \texttt{CVodesIntegrator} is actually a special function, it is natural to expect first-order derivatives from it. Of course the underlying CVODES integrator must
provide sensitivities with respect to the initial value and the parameters used in the equations. We extend our previous source code and explain only the relevant parts.
\par
\codebegin
\begin{verbatim}
     1  CVodesIntegrator integrator(ode_func, quad_func);
     2  integrator.setOption("number_of_fwd_dir", 1);
     3  integrator.setOption("number_of_adj_dir", 1);
     4  integrator.init();
     5  integrator.setInput(0.0, INTEGRATOR_T0);
     6  integrator.setInput(1.0, INTEGRATOR_TF);
     7  vector<double> x0(3 + 2);
     8  fill(x0.begin(), x0.end(), 0.0);
     9  x0[0] = 0; x0[1] = 1; x0[2] = 10;
    10  integrator.setInput(x0, INTEGRATOR_X0);
    11  vector<double> p(1);
    12  p[0] = 0.15;
    13  integrator.setInput(p, INTEGRATOR_P);
    14  vector<double> fwd_seed_x(3 + 2);
    15  fill(fwd_seed_x.begin(), fwd_seed_x.end(), 1);
    16  vector<double> fwd_seed_p(1);
    17  fwd_seed_p[0] = 10;
    18  integrator.setFwdSeed(fwd_seed_x, INTEGRATOR_X0);
    19  integrator.setFwdSeed(fwd_seed_p, INTEGRATOR_P);
    20  vector<double> adj_seed(3 + 2);
    21  fill(adj_seed.begin(), adj_seed.end(), 4.0);
    22  integrator.setAdjSeed(adj_seed, INTEGRATOR_XF);
    23  integrator.evaluate(1, 1);
    24  vector<double> xf = integrator.output(INTEGRATOR_XF);
    25  vector<double> forward_derivative = integrator.fwdSens(INTEGRATOR_XF);
    26  vector<double> adjoint_derivative_x = integrator.adjSens(INTEGRATOR_X0);
    27  vector<double> adjoint_derivative_p = integrator.adjSens(INTEGRATOR_P);
\end{verbatim}
\codeend\\
%\end{rawhtml}
%\par
Similary as we did in case of plain functions, we have to set the number of directional derivatives (line 2-3). In
forward mode more than one direction is supported. The forward seed we have to set with respect to the dynamic variables and the parameters as well 
(see line 18-19). The way that the adjoint seed must be set comes from the behaviour of CVODES, namely that it does not give possibility to 
calculate adjoint derivatives, but the feature of integrating backwards. Thus, the adjoint seed is an "initial value" at the final time 
and should be indexed with \texttt{INTEGRATOR\_XF} (see line 22). Gathering the derivatives is just the other way around. Forward derivatives can
be collected at the final time (line 25), while adjoints are available at the initial time in two pieces corresponding to dynamic variables and parameters.
\subsection*{Integrator options}
In the following subsection we would like to introduce the most important integrator options. Most of the options are directly available and usable
 known from the CVODES documentation, altough this is very detailed and quite often one cannot find what he is looking for. One can get the 
 list of actual options by invoking the \texttt{printOptions()} method of the \texttt{CVodesIntegrator} object.
 \begin{center}
 \small
 \begin{tabular}{|m{5.3cm}|m{2.3cm}|m{17cm}|}
 \hline
 \textbf{Option} & \textbf{Value} & \textbf{Description}\\
 \hline
 \hline
  \texttt{linear\_multistep\_method} & bdf, adams & Defines which implicit integration approach to be used. BDF is the default, 
 \texttt{adams} means  Adams-Moulton method.\\
 \hline
 \texttt{number\_of\_fwd\_dir} & int & Sets the number of forward directional derivatives.\\
 \hline
 \texttt{number\_of\_adj\_dir} & int & Sets the number of backward directional derivatives (can't be more than 1).\\
 \hline
 \texttt{fsens\_err\_con} & bool & Activates forward sensitivity error control, if false sensitivity equations are not considered while
 determining the step size. By default it is disabled. \\
 \hline
 \texttt{quad\_err\_con} & bool & Activates quadrature error control, if false quadrature equations are not considered while determining
 the step size. By default it is disabled.\\
 \hline 
 \texttt{linear\_solver} & dense, iterative, banded, user\_defined & Determines linear solver and Jacobian approximation to be used in Newton iterations. Default is \texttt{dense},
 whereas \texttt{iterative} is one of the Krylov solvers (see option \texttt{iterative\_solver} for details) and
 \texttt{banded} approxmiation or  \texttt{user\_defined} are also possible.\\
 \hline
 \texttt{iterative\_solver} & gmres, bcgstab, tfqmr & Sets which iterative Krylov solver to be used (see option \texttt{linear\_solver}). 
 The default is gmres.  \\
 \hline
 \texttt{exact\_jacobian} & bool & Activates exact Jacobian computation in Newton iterations. If false, which is the default, an approximation is deployed.\\
 \hline
 \texttt{max\_num\_steps} & int & Defines the maximum number of steps that the integrator may take (default: 10000).\\
 \hline
 \texttt{abstol}& double & Sets the absolute tolerance (default: 1E-08).\\
 \hline
 \texttt{reltol}& double & Sets the relative tolerance (default: 1E-06).\\
 \hline
 \texttt{nonlinear\_solver\_iteration} & newton, functional & Choose nonlinear solver, either Newton's method, which is the default
 or Functional iteration.  \\
 \hline
 \texttt{fsens\_all\_at\_once} & bool & If true the right-hand side of all sensitivity equations are evaluated, default is true. \\
 \hline
 \texttt{pretype} & none, left, right, both & Determines the type of preconditioning of linear systems, by default it is \texttt{none}.  \\
 \hline
 \texttt{sensitivity\_method} &  simultaneous, staggered & Determines how forward sensitivities are calculated. 
 Simultaneous corrector method is the default, staggered corrector approach is an option. \\
 \hline
 \texttt{interpolation\_type} & hermite, polynomial & Determines interpolation method for backward derivatives, candidates are 
 variable-degree polynomial and cubic Hermite interpolation (default).  \\
 \hline
 \texttt{asens\_linear\_solver} & dense, banded, iterative & Determines linear solver and Jacobian approximation to be used in Newton iterations
 for backward sensitivities. \\
 \hline
 \texttt{asens\_iterative\_solver} & gmres, bcgstab, tfqmr & Sets which iterative Krylov solver to be used for backward sensitivities.\\
 \hline
 \end{tabular}
 \end{center}
\section{Integration of implicit ODEs}
\chapter{Code generation}
%\chapter{Second-order derivatives}
\chapter{Nonlinear Program Solvers}
%\bibliographystyle{plain}
%\bibliography{optec}
\end{document}